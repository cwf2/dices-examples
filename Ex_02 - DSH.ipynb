{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"well\" style=\"margin:1em 2em\">\n",
    "<p>This Notebook reproduces and expands on a demo from “Distant Reading of Direct Speech in Epic: An Illustrated Workflow,” a talk I gave at the FIEC / CA annual meeting in London, July 8, 2019.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Heroes and their moms\n",
    "\n",
    "Let's say we're young scholars interested in Telemachus' speech to Penelope.\n",
    " - How often does he speak to her?\n",
    " - What kind of language does he use?\n",
    " - How does the narrator refer to these speeches?\n",
    " \n",
    "We'll start by showing how the DICES database and Python library can be used to retrieve and manipulate the speeches in question. Then we'll expand our perspective to show how DICES enables research on a \"distant reading\" scale, taking in all heroes and their mothers. Finally, we'll check the accuracy of the automated methods by comparing against a benchmark of hand-curated mother-child speech data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the client library\n",
    "\n",
    "If you don't have the DICES client library, you can install it with **pip**:\n",
    "```\n",
    "pip install git+https://github.com/cwf2/dices-client.git\n",
    "```\n",
    "\n",
    "### The DICES API\n",
    "\n",
    "When you instantiate the API, you can optionally provide endpoints for the DICES database and for a CTS server hosting the texts.\n",
    "\n",
    "- The default endpoint for DICES is our Heroku development instance; it runs a little slow, especially if it hasn't been used in a while.\n",
    "\n",
    "- The default for texts is the [Perseids CTS server](https://cts.perseids.org/).\n",
    "\n",
    "Finally, just for Jupyter, I'm passing an optional progress bar generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "\n",
    "api = DicesAPI(progress_class=NotebookPBar, logfile='dices.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK\n",
    "\n",
    "#### Make sure the corpora are present\n",
    "\n",
    "These only have to be downloaded once in a given Python environemnt. I'm including this here because I use this notebook with Binder, and everything has to be installed from scratch each time I run it. For other applications, you might want additional corpora; I've commented them out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "from cltk.alphabet.text_normalization import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up tokenizers, lemmatizers\n",
    "\n",
    "I like to have one convenience function that I can call on every speech, regardless of language. That means I have to set up language-specific tokenizers and lemmatizers first, and also cook up some kludgey regular expression substitutions to normalize orthography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‎𐤀 CLTK version '1.0.21'.\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekStanzaProcess`, `GreekEmbeddingsProcess`, `StopsProcess`, `GreekNERProcess`.\n",
      "‎𐤀 CLTK version '1.0.21'.\n",
      "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinStanzaProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinNERProcess`, `LatinLexiconProcess`.\n"
     ]
    }
   ],
   "source": [
    "cltk_nlp = {\n",
    "    'greek': NLP(language='grc'),\n",
    "    'latin': NLP(language='lat'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData\n",
    "\n",
    "To figure out how characters are related, which isn't in the DICES metadata, I'm going to use [WikiData](https://www.wikidata.org), via the *qwikidata* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1\n",
    "\n",
    "Let's start by building a lexicon for all the words Telemachus speaks to Penelope.\n",
    "\n",
    "### Identify and download the speeches\n",
    "\n",
    "Using the hand-rolled DICES API code, we can search speeches using keywords. For now, JSON results from the API are paged, so if your search has a lot of results, you may have to wait for several pages to download. I've added a progress bar widget because I get impatient.\n",
    "\n",
    "Note that I can specify both the speaker and the addressee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fef2ab194fd49f9ae05507e4704a05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=6), Label(value='0/6')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches = api.getSpeeches(spkr_name='Telemachus', addr_name='Penelope', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Speech 713: Odyssey 1.346-1.359>\n",
      "<Speech 1103: Odyssey 17.46-17.56>\n",
      "<Speech 1107: Odyssey 17.108-17.149>\n",
      "<Speech 1170: Odyssey 18.227-18.242>\n",
      "<Speech 1270: Odyssey 21.344-21.353>\n",
      "<Speech 1323: Odyssey 23.97-23.103>\n"
     ]
    }
   ],
   "source": [
    "for s in speeches:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the passages from a remote library\n",
    "\n",
    "We have the metadata for each speech; now we need the text. The DICES library uses [MyCapytain](https://mycapytain.readthedocs.io) under the hood to retrieve the passages from a remote CTS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homer Odyssey 1.346-1.359\n",
      "μῆτερ ἐμή, τί τʼ ἄρα φθονέεις ἐρίηρον ἀοιδὸν τέρπειν ὅππῃ οἱ νόος ὄρνυται; οὔ νύ τʼ ἀοιδοὶ αἴτιοι, ἀλλά ποθι Ζεὺς αἴτιος, ὅς τε δίδωσιν ἀνδράσιν ἀλφηστῇσιν, ὅπως ἐθέλῃσιν, ἑκάστῳ. τούτῳ δʼ οὐ νέμεσις Δαναῶν κακὸν οἶτον ἀείδειν· τὴν γὰρ ἀοιδὴν μᾶλλον ἐπικλείουσʼ ἄνθρωποι, ἥ τις ἀκουόντεσσι νεωτάτη ἀμφιπέληται. σοὶ δʼ ἐπιτολμάτω κραδίη καὶ θυμὸς ἀκούειν· οὐ γὰρ Ὀδυσσεὺς οἶος ἀπώλεσε νόστιμον ἦμαρ ἐν Τροίῃ, πολλοὶ δὲ καὶ ἄλλοι φῶτες ὄλοντο. ἀλλʼ εἰς οἶκον ἰοῦσα τὰ σʼ αὐτῆς ἔργα κόμιζε, ἱστόν τʼ ἠλακάτην τε, καὶ ἀμφιπόλοισι κέλευε ἔργον ἐποίχεσθαι· μῦθος δʼ ἄνδρεσσι μελήσει πᾶσι, μάλιστα δʼ ἐμοί· τοῦ γὰρ κράτος ἔστʼ ἐνὶ οἴκῳ.\n",
      "\n",
      "Homer Odyssey 17.46-17.56\n",
      "μῆτερ ἐμή, μή μοι γόον ὄρνυθι μηδέ μοι ἦτορ ἐν στήθεσσιν ὄρινε φυγόντι περ αἰπὺν ὄλεθρον· ἀλλʼ ὑδρηναμένη, καθαρὰ χροῒ εἵμαθʼ ἑλοῦσα, εἰς ὑπερῷʼ ἀναβᾶσα σὺν ἀμφιπόλοισι γυναιξὶν εὔχεο πᾶσι θεοῖσι τεληέσσας ἑκατόμβας ῥέξειν, αἴ κέ ποθι Ζεὺς ἄντιτα ἔργα τελέσσῃ. αὐτὰρ ἐγὼν ἀγορὴν ἐσελεύσομαι, ὄφρα καλέσσω ξεῖνον, ὅτις μοι κεῖθεν ἅμʼ ἕσπετο δεῦρο κιόντι. τὸν μὲν ἐγὼ προὔπεμψα σὺν ἀντιθέοις ἑτάροισι, Πείραιον δέ μιν ἠνώγεα προτὶ οἶκον ἄγοντα ἐνδυκέως φιλέειν καὶ τιέμεν, εἰς ὅ κεν ἔλθω.\n",
      "\n",
      "Homer Odyssey 17.108-17.149\n",
      "τοιγὰρ ἐγώ τοι, μῆτερ, ἀληθείην καταλέξω. ᾠχόμεθʼ ἔς τε Πύλον καὶ Νέστορα, ποιμένα λαῶν· δεξάμενος δέ με κεῖνος ἐν ὑψηλοῖσι δόμοισιν ἐνδυκέως ἐφίλει, ὡς εἴ τε πατὴρ ἑὸν υἱὸν ἐλθόντα χρόνιον νέον ἄλλοθεν· ὣς ἐμὲ κεῖνος ἐνδυκέως ἐκόμιζε σὺν υἱάσι κυδαλίμοισιν. αὐτὰρ Ὀδυσσῆος ταλασίφρονος οὔ ποτʼ ἔφασκεν, ζωοῦ οὐδὲ θανόντος, ἐπιχθονίων τευ ἀκοῦσαι· ἀλλά μʼ ἐς Ἀτρεΐδην, δουρικλειτὸν Μενέλαον, ἵπποισι προὔπεμψε καὶ ἅρμασι κολλητοῖσιν. ἔνθʼ ἴδον Ἀργείην Ἑλένην, ἧς εἵνεκα πολλὰ Ἀργεῖοι Τρῶές τε θεῶν ἰότητι μόγησαν. εἴρετο δʼ αὐτίκʼ ἔπειτα βοὴν ἀγαθὸς Μενέλαος ὅττευ χρηΐζων ἱκόμην Λακεδαίμονα δῖαν· αὐτὰρ ἐγὼ τῷ πᾶσαν ἀληθείην κατέλεξα· καὶ τότε δή με ἔπεσσιν ἀμειβόμενος προσέειπεν· ὢ πόποι, ἦ μάλα δὴ κρατερόφρονος ἀνδρὸς ἐν\n",
      " εὐνῇ ἤθελον εὐνηθῆναι, ἀνάλκιδες αὐτοὶ ἐόντες. ὡς δʼ ὁπότʼ ἐν ξυλόχῳ ἔλαφος κρατεροῖο λέοντος νεβροὺς κοιμήσασα νεηγενέας γαλαθηνοὺς κνημοὺς ἐξερέῃσι καὶ ἄγκεα ποιήεντα βοσκομένη, ὁ δʼ ἔπειτα ἑὴν εἰσήλυθεν εὐνήν, ἀμφοτέροισι δὲ τοῖσιν ἀεικέα πότμον ἐφῆκεν, ὣς Ὀδυσεὺς κείνοισιν ἀεικέα πότμον ἐφήσει. αἲ γάρ, Ζεῦ τε πάτερ καὶ Ἀθηναίη καὶ Ἄπολλον, τοῖος ἐὼν οἷός ποτʼ ἐϋκτιμένῃ ἐνὶ Λέσβῳ ἐξ ἔριδος Φιλομηλεΐδῃ ἐπάλαισεν ἀναστάς, κὰδ δʼ ἔβαλε κρατερῶς, κεχάροντο δὲ πάντες Ἀχαιοί, τοῖος ἐὼν μνηστῆρσιν ὁμιλήσειεν Ὀδυσσεύς· πάντες κʼ ὠκύμοροί τε γενοίατο πικρόγαμοί τε. ταῦτα δʼ ἅ μʼ εἰρωτᾷς καὶ λίσσεαι, οὐκ ἂν ἐγώ γε ἄλλα παρὲξ εἴποιμι παρακλιδὸν οὐδʼ ἀπατήσω, ἀλλὰ τὰ μέν μοι ἔειπε γέρων ἅλιος νημερτής, τῶν οὐδέν τοι ἐγὼ κρύψω ἔπος οὐδʼ ἐπικεύσω. φῆ μιν ὅ γʼ ἐν νήσῳ ἰδέειν κρατέρʼ ἄλγεʼ ἔχοντα, νύμφης ἐν μεγάροισι Καλυψοῦς, ἥ μιν ἀνάγκῃ ἴσχει· ὁ δʼ οὐ δύναται ἣν πατρίδα γαῖαν ἱκέσθαι. οὐ γάρ οἱ πάρα νῆες ἐπήρετμοι καὶ ἑταῖροι, οἵ κέν μιν πέμποιεν ἐπʼ εὐρέα νῶτα θαλάσσης ὣς ἔφατʼ Ἀτρεΐδης, δουρικλειτὸς Μενέλαος. ταῦτα τελευτήσας νεόμην· ἔδοσαν δέ μοι οὖρον ἀθάνατοι, τοί μʼ ὦκα φίλην ἐς πατρίδʼ ἔπεμψαν.\n",
      "\n",
      "Homer Odyssey 18.227-18.242\n",
      "μῆτερ ἐμή, τὸ μὲν οὔ σε νεμεσσῶμαι κεχολῶσθαι· αὐτὰρ ἐγὼ θυμῷ νοέω καὶ οἶδα ἕκαστα, ἐσθλά τε καὶ τὰ χέρεια· πάρος δʼ ἔτι νήπιος ἦα. ἀλλά τοι οὐ δύναμαι πεπνυμένα πάντα νοῆσαι· ἐκ γάρ με πλήσσουσι παρήμενοι ἄλλοθεν ἄλλος οἵδε κακὰ φρονέοντες, ἐμοὶ δʼ οὐκ εἰσὶν ἀρωγοί. οὐ μέν τοι ξείνου γε καὶ Ἴρου μῶλος ἐτύχθη μνηστήρων ἰότητι, βίῃ δʼ ὅ γε φέρτερος ἦεν. αἲ γάρ, Ζεῦ τε πάτερ καὶ Ἀθηναίη καὶ Ἄπολλον, οὕτω νῦν μνηστῆρες ἐν ἡμετέροισι δόμοισι νεύοιεν κεφαλὰς δεδμημένοι, οἱ μὲν ἐν αὐλῇ, οἱ δʼ ἔντοσθε δόμοιο, λελῦτο δὲ γυῖα ἑκάστου, ὡς νῦν Ἶρος κεῖνος ἐπʼ αὐλείῃσι θύρῃσιν ἧσται νευστάζων κεφαλῇ, μεθύοντι ἐοικώς, οὐδʼ ὀρθὸς στῆναι δύναται ποσὶν οὐδὲ νέεσθαι οἴκαδʼ, ὅπη οἱ νόστος, ἐπεὶ φίλα γυῖα λέλυνται.\n",
      "\n",
      "Homer Odyssey 21.344-21.353\n",
      "μῆτερ ἐμή, τόξον μὲν Ἀχαιῶν οὔ τις ἐμεῖο κρείσσων, ᾧ κʼ ἐθέλω, δόμεναί τε καὶ ἀρνήσασθαι, οὔθʼ ὅσσοι κραναὴν Ἰθάκην κάτα κοιρανέουσιν, οὔθʼ ὅσσοι νήσοισι πρὸς Ἤλιδος ἱπποβότοιο· τῶν οὔ τίς μʼ ἀέκοντα βιήσεται, αἴ κʼ ἐθέλωμι καὶ καθάπαξ ξείνῳ δόμεναι τάδε τόξα φέρεσθαι. ἀλλʼ εἰς οἶκον ἰοῦσα τὰ σʼ αὐτῆς ἔργα κόμιζε, ἱστόν τʼ ἠλακάτην τε, καὶ ἀμφιπόλοισι κέλευε ἔργον ἐποίχεσθαι· τόξον δʼ ἄνδρεσσι μελήσει πᾶσι, μάλιστα δʼ ἐμοί· τοῦ γὰρ κράτος ἔστʼ ἐνὶ οἴκῳ.\n",
      "\n",
      "Homer Odyssey 23.97-23.103\n",
      "μῆτερ ἐμή, δύσμητερ, ἀπηνέα θυμὸν ἔχουσα, τίφθʼ οὕτω πατρὸς νοσφίζεαι, οὐδὲ παρʼ αὐτὸν ἑζομένη μύθοισιν ἀνείρεαι οὐδὲ μεταλλᾷς; οὐ μέν κʼ ἄλλη γʼ ὧδε γυνὴ τετληότι θυμῷ ἀνδρὸς ἀφεσταίη, ὅς οἱ κακὰ πολλὰ μογήσας ἔλθοι ἐεικοστῷ ἔτεϊ ἐς πατρίδα γαῖαν· σοὶ δʼ αἰεὶ κραδίη στερεωτέρη ἐστὶ λίθοιο.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passages = []\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    text = cts_passage.text\n",
    "    passages.append(text)\n",
    "    \n",
    "    print(f'{s.author.name} {s.work.title} {s.l_range}')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CLTK to parse the text\n",
    "\n",
    "We can use CTLK's tokenizers to break each string into meaningful units -- sentences and/or words. Then we use the backoff lemmatizer to normalize all the inflected forms to dictionary headwords.\n",
    "\n",
    "I rolled these steps into one convenience function up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feat_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_k/nhmmjzg96r318bm9jvyvv7ph0000gn/T/ipykernel_58561/205355683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpassages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcltk_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcltk_nlp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     these_lems = [lem for tok, lem in lemmatized]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/nlp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/nlp.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0ma_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_process_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/processes.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_doc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moutput_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mstanza_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/boltons/cacheutils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/processes.py\u001b[0m in \u001b[0;36malgorithm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcachedproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mStanzaWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_doc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36mget_nlp\u001b[0;34m(cls, language, treebank)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, treebank, stanza_debug_level, interactive, silent)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# for the log file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36m_load_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlogging_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanza_debug_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# default, won't fail if GPU not present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mlemma_use_identity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemma_use_identity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         )\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[1;32m    142\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m# if there was a requirements issue, add it to list which will be printed at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_variant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# build the final config for the processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pre_tokenized_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# load everything from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# build model from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# were built with mwt layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_mwt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emb_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat_dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feat_dropout'"
     ]
    }
   ],
   "source": [
    "lems = Counter()\n",
    "for p in passages:\n",
    "    lang = s.work.lang\n",
    "    cltk_doc = cltk_nlp[lang](p)\n",
    "    \n",
    "#     these_lems = [lem for tok, lem in lemmatized]\n",
    "#     lems.update(these_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the counter to a Pandas data frame for tidier presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lems.most_common(), columns=['lemma', 'count'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now let's think more broadly. How typical is this kind of speech? We can use external linked data to find other examples of mother-son conversations in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom code to query WikiData\n",
    "\n",
    "This lets us ask whether a given addressee belongs to the set of people having a certain relationship to a given speaker. It takes a while to download the WikiData entities, and I had to run this a number of times, so I cached WD data in the respective character objects once it's downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWD(c):\n",
    "    '''make sure character has wikidata id'''\n",
    "    if c.char is not None:\n",
    "        if c.char.wd is not None:\n",
    "            if len(c.char.wd.strip()) > 0:\n",
    "                return c.char.wd.strip()\n",
    "\n",
    "def checkWDRelation(s, a, relation, cache=None):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    else:\n",
    "        if (s.id, a.id) in cache:\n",
    "            return cache[(s.id, a.id)]\n",
    "\n",
    "    res = False\n",
    "\n",
    "    if not hasattr(s, 'wd_ent'):\n",
    "        s.wd_ent = WikidataItem(get_entity_dict_from_api(s.wd))\n",
    "\n",
    "    claim_group = s.wd_ent.get_truthy_claim_group(relation)\n",
    "\n",
    "    for claim in claim_group:\n",
    "        if claim.mainsnak.datavalue is None:\n",
    "            continue\n",
    "        if claim.mainsnak.datavalue.value['id'] == a.wd:\n",
    "            res = True\n",
    "    \n",
    "    cache[(s.id, a.id)] = res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the relation \"mother of\" has the WikiData ID `'P25'`. Here's how we ask if a given addressee is the mother of a given speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Telemachus')[0]\n",
    "addressee = api.getCharacters(name='Penelope')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added a separate cache just for the boolean result of checkWDRelation, to save a little more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_mothers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WikiData to filter the speeches\n",
    "\n",
    "The DICES dataset includes WikiData ids for most of the characters (not all). The DICES API doesn't let us query WikiData itself, though. For now, the easiest thing is just to download all the speeches and character IDs, and then cross reference them against WikiData using its own API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the speeches: takes a minute\n",
    "speeches = (\n",
    "    api.getSpeeches(author_name='Homer', progress=True) +\n",
    "    api.getSpeeches(author_name='Apollonius', progress=True) +\n",
    "    api.getSpeeches(author_name='Virgil', progress=True))\n",
    "\n",
    "speeches.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check each speaker-addressee pair against WikiData**\n",
    "\n",
    "What we actually do here is download the WikiData entity for each speaker, if we don't already have it cached. Then we ask the WD entity for its mom(s), and check the WD ID of the addressee against the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty table\n",
    "rows = []\n",
    "\n",
    "# create a progress bar\n",
    "pbar = NotebookPBar(start=0, max=len(speeches))\n",
    "\n",
    "# iterate over all the speeches, checking each speaker-addressee combination\n",
    "for s in speeches:\n",
    "    if s.spkr is not None and s.addr is not None:\n",
    "        for spkr in s.spkr:\n",
    "            spkr_wd = checkWD(spkr)\n",
    "            if spkr_wd is not None:\n",
    "\n",
    "                for addr in s.addr:\n",
    "                    addr_wd = checkWD(addr)\n",
    "                    if addr_wd is not None:\n",
    "                        rows.append((\n",
    "                            s.id,\n",
    "                            s.work.title,\n",
    "                            s.l_fi,\n",
    "                            s.l_la,\n",
    "                            spkr.char.name, spkr_wd, \n",
    "                            addr.char.name, addr_wd,\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P25', cache=cache_mothers),\n",
    "                            checkWDRelation(addr.char, spkr.char, 'P25', cache=cache_mothers)\n",
    "                            ))\n",
    "    pbar.update()\n",
    "\n",
    "# finally, organize the table as a pandas data frame\n",
    "df = pd.DataFrame(rows, columns=['id', 'work', 'l_first', 'l_last', 'spkr', 'sp_wd', 'addr', 'ad_wd', 'sp_is_mom', 'ad_is_mom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 Let's take a look at the results. Here is the complete set of speeches, with the additional attribute `sp_is_mom` if the speaker is the addressee's mother, and `ad_is_mom` if the addressee is the speaker's mother.\n",
    "\n",
    "As a quick sanity check, the first two speeches in the Argonautica, which were at the top of the list when I ran this, are between Jason and his mother, Alcimede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['work']=='Argonautica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to pandas, we can filter the data frame on the new boolean columns to show only speeches between mother and child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = df.loc[df['sp_is_mom'] | df['ad_is_mom'],\n",
    "             ['work', 'l_first', 'l_last', 'spkr', 'addr']]\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also comes in handy if I wanted to export this data to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Let's see how well the automated approach worked. We'll load up a hand-corrected list of mother-child speeches and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('data/moms-bench.csv', dtype=str)\n",
    "bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the union of `hits` and `bench` to see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hits.merge(bench, on=['work', 'l_first'], how='outer', \n",
    "                        suffixes=['_h', '_b'], indicator=True)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(results[['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b', '_merge']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = sum(results['_merge'] == 'both')\n",
    "\n",
    "p = true_pos / hits.shape[0]\n",
    "r = true_pos / bench.shape[0]\n",
    "\n",
    "print(f'Precision: {p:.2f}')\n",
    "print(f'Recall:    {r:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### The good news\n",
    "We got almost all of the benchmark set, with exceptions to be discussed below, and no false positives.\n",
    "\n",
    "#### The bad news\n",
    "Let's look a little more closely at the speeches we missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = results[results['_merge'] == 'right_only'][\n",
    "                ['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b']]\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, I'd say these fall into three groups:\n",
    "\n",
    " 1. A conversation in the Iliad between Hera and a group of gods, some of whom were here children\n",
    " 2. A conversation in the Aeneid between Jupiter and \"Cybele,\" i.e., Rhea.\n",
    " 3. A conversation in the Aeneid between Euryalus and his anonymous mother.\n",
    "\n",
    "#### Digging a little deeper\n",
    "\n",
    "First, let's confirm that all these speeches are in the database results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed.merge(df, how='left', on=['work', 'l_first'])[[\n",
    "    'work', 'l_first',                               # keys: work and locus\n",
    "    'id', 'spkr', 'addr', 'sp_is_mom', 'ad_is_mom',  # cols from df\n",
    "    'spkr_b', 'addr_b'                               # cols from bench\n",
    "    \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech between Euyalus and his mom is missing from the database. That's because as of this writing we don't have a systematic way of including anonymous characters like her--folks described only by a family relation or an occupation. Because the character doesn't fit our current data model, she gets omitted, and this speech fails to be added to the database.\n",
    "\n",
    "The conversation between Hera and the gods is there in the database, but the speaker-addressee pairs are not registering as mother-child relationships. In this case, it's because \"gods\" isn't being parsed as including all the individual gods, but rather a corporate entity that doesn't have \"mother\" or \"child\" as properties. This highlights another issue that needs to be resolved in our data model.\n",
    "\n",
    "Finally, the conversation between Jupiter and his mom is also in the database, and each of these characters is matched with a WikiData entity, but we're not getting the right answer about their kinship relation because WikiData has distinct entities for the Greek goddess Rhea and the Phrygian goddess Cybele. We can fix this by pointing the character's WikiData ID to the Greek goddess instead (as we did for the Roman deities), but maybe we should think about the larger problem of poetic ambiguity/metonymy/syncretism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    " - WikiData gave us a lot for free -- all of the individual mother-child relationships were in there when we knew where to look.\n",
    " \n",
    " - There is still some important work to be done refining our underlying data model.\n",
    " \n",
    " - If we want to rely on linked open data for high-stakes work, we need resources that are sensitive to the details we care about. We hope that MANTO, because it's specific to Classical myth and hand-curated by domain experts, can help us with problems like when to treat Cybele and Rhea as independent entities and when to consider them identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
