{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"well\" style=\"margin:1em 2em\">\n",
    "<p>This Notebook reproduces and expands on a demo from â€œDistant Reading of Direct Speech in Epic: An Illustrated Workflow,â€ a talk I gave at the FIEC / CA annual meeting in London, July 8, 2019.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Heroes and their moms\n",
    "\n",
    "Let's say we're young scholars interested in Telemachus' speech to Penelope.\n",
    " - How often does he speak to her?\n",
    " - What kind of language does he use?\n",
    " - How does the narrator refer to these speeches?\n",
    " \n",
    "We'll start by showing how the DICES database and Python library can be used to retrieve and manipulate the speeches in question. Then we'll expand our perspective to show how DICES enables research on a \"distant reading\" scale, taking in all heroes and their mothers. Finally, we'll check the accuracy of the automated methods by comparing against a benchmark of hand-curated mother-child speech data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the client library\n",
    "\n",
    "If you don't have the DICES client library, you can install it with **pip**:\n",
    "```\n",
    "pip install git+https://github.com/cwf2/dices-client.git\n",
    "```\n",
    "\n",
    "### The DICES API\n",
    "\n",
    "When you instantiate the API, you can optionally provide endpoints for the DICES database and for a CTS server hosting the texts.\n",
    "\n",
    "- The default endpoint for DICES is our Heroku development instance; it runs a little slow, especially if it hasn't been used in a while.\n",
    "\n",
    "- The default for texts is the [Perseids CTS server](https://cts.perseids.org/).\n",
    "\n",
    "Finally, just for Jupyter, I'm passing an optional progress bar generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "\n",
    "api = DicesAPI(progress_class=NotebookPBar, logfile='dices.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK\n",
    "\n",
    "#### Make sure the corpora are present\n",
    "\n",
    "These only have to be downloaded once in a given Python environemnt. I'm including this here because I use this notebook with Binder, and everything has to be installed from scratch each time I run it. For other applications, you might want additional corpora; I've commented them out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP\n",
    "from cltk.alphabet.text_normalization import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up tokenizers, lemmatizers\n",
    "\n",
    "I like to have one convenience function that I can call on every speech, regardless of language. That means I have to set up language-specific tokenizers and lemmatizers first, and also cook up some kludgey regular expression substitutions to normalize orthography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€ğ¤€ CLTK version '1.0.21'.\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekStanzaProcess`, `GreekEmbeddingsProcess`, `StopsProcess`, `GreekNERProcess`.\n",
      "â€ğ¤€ CLTK version '1.0.21'.\n",
      "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinStanzaProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinNERProcess`, `LatinLexiconProcess`.\n"
     ]
    }
   ],
   "source": [
    "cltk_nlp = {\n",
    "    'greek': NLP(language='grc'),\n",
    "    'latin': NLP(language='lat'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData\n",
    "\n",
    "To figure out how characters are related, which isn't in the DICES metadata, I'm going to use [WikiData](https://www.wikidata.org), via the *qwikidata* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1\n",
    "\n",
    "Let's start by building a lexicon for all the words Telemachus speaks to Penelope.\n",
    "\n",
    "### Identify and download the speeches\n",
    "\n",
    "Using the hand-rolled DICES API code, we can search speeches using keywords. For now, JSON results from the API are paged, so if your search has a lot of results, you may have to wait for several pages to download. I've added a progress bar widget because I get impatient.\n",
    "\n",
    "Note that I can specify both the speaker and the addressee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fef2ab194fd49f9ae05507e4704a05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=6), Label(value='0/6')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speeches = api.getSpeeches(spkr_name='Telemachus', addr_name='Penelope', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Speech 713: Odyssey 1.346-1.359>\n",
      "<Speech 1103: Odyssey 17.46-17.56>\n",
      "<Speech 1107: Odyssey 17.108-17.149>\n",
      "<Speech 1170: Odyssey 18.227-18.242>\n",
      "<Speech 1270: Odyssey 21.344-21.353>\n",
      "<Speech 1323: Odyssey 23.97-23.103>\n"
     ]
    }
   ],
   "source": [
    "for s in speeches:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the passages from a remote library\n",
    "\n",
    "We have the metadata for each speech; now we need the text. The DICES library uses [MyCapytain](https://mycapytain.readthedocs.io) under the hood to retrieve the passages from a remote CTS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homer Odyssey 1.346-1.359\n",
      "Î¼á¿†Ï„ÎµÏ á¼Î¼Î®, Ï„Î¯ Ï„Ê¼ á¼„ÏÎ± Ï†Î¸Î¿Î½Î­ÎµÎ¹Ï‚ á¼ÏÎ¯Î·ÏÎ¿Î½ á¼€Î¿Î¹Î´á½¸Î½ Ï„Î­ÏÏ€ÎµÎ¹Î½ á½…Ï€Ï€á¿ƒ Î¿á¼± Î½ÏŒÎ¿Ï‚ á½„ÏÎ½Ï…Ï„Î±Î¹; Î¿á½” Î½Ï Ï„Ê¼ á¼€Î¿Î¹Î´Î¿á½¶ Î±á¼´Ï„Î¹Î¿Î¹, á¼€Î»Î»Î¬ Ï€Î¿Î¸Î¹ Î–Îµá½ºÏ‚ Î±á¼´Ï„Î¹Î¿Ï‚, á½…Ï‚ Ï„Îµ Î´Î¯Î´Ï‰ÏƒÎ¹Î½ á¼€Î½Î´ÏÎ¬ÏƒÎ¹Î½ á¼€Î»Ï†Î·ÏƒÏ„á¿‡ÏƒÎ¹Î½, á½…Ï€Ï‰Ï‚ á¼Î¸Î­Î»á¿ƒÏƒÎ¹Î½, á¼‘ÎºÎ¬ÏƒÏ„á¿³. Ï„Î¿ÏÏ„á¿³ Î´Ê¼ Î¿á½ Î½Î­Î¼ÎµÏƒÎ¹Ï‚ Î”Î±Î½Î±á¿¶Î½ ÎºÎ±Îºá½¸Î½ Î¿á¼¶Ï„Î¿Î½ á¼€ÎµÎ¯Î´ÎµÎ¹Î½Â· Ï„á½´Î½ Î³á½°Ï á¼€Î¿Î¹Î´á½´Î½ Î¼á¾¶Î»Î»Î¿Î½ á¼Ï€Î¹ÎºÎ»ÎµÎ¯Î¿Ï…ÏƒÊ¼ á¼„Î½Î¸ÏÏ‰Ï€Î¿Î¹, á¼¥ Ï„Î¹Ï‚ á¼€ÎºÎ¿Ï…ÏŒÎ½Ï„ÎµÏƒÏƒÎ¹ Î½ÎµÏ‰Ï„Î¬Ï„Î· á¼€Î¼Ï†Î¹Ï€Î­Î»Î·Ï„Î±Î¹. ÏƒÎ¿á½¶ Î´Ê¼ á¼Ï€Î¹Ï„Î¿Î»Î¼Î¬Ï„Ï‰ ÎºÏÎ±Î´Î¯Î· ÎºÎ±á½¶ Î¸Ï…Î¼á½¸Ï‚ á¼€ÎºÎ¿ÏÎµÎ¹Î½Â· Î¿á½ Î³á½°Ï á½ˆÎ´Ï…ÏƒÏƒÎµá½ºÏ‚ Î¿á¼¶Î¿Ï‚ á¼€Ï€ÏÎ»ÎµÏƒÎµ Î½ÏŒÏƒÏ„Î¹Î¼Î¿Î½ á¼¦Î¼Î±Ï á¼Î½ Î¤ÏÎ¿Î¯á¿ƒ, Ï€Î¿Î»Î»Î¿á½¶ Î´á½² ÎºÎ±á½¶ á¼„Î»Î»Î¿Î¹ Ï†á¿¶Ï„ÎµÏ‚ á½„Î»Î¿Î½Ï„Î¿. á¼€Î»Î»Ê¼ Îµá¼°Ï‚ Î¿á¼¶ÎºÎ¿Î½ á¼°Î¿á¿¦ÏƒÎ± Ï„á½° ÏƒÊ¼ Î±á½Ï„á¿†Ï‚ á¼”ÏÎ³Î± ÎºÏŒÎ¼Î¹Î¶Îµ, á¼±ÏƒÏ„ÏŒÎ½ Ï„Ê¼ á¼ Î»Î±ÎºÎ¬Ï„Î·Î½ Ï„Îµ, ÎºÎ±á½¶ á¼€Î¼Ï†Î¹Ï€ÏŒÎ»Î¿Î¹ÏƒÎ¹ ÎºÎ­Î»ÎµÏ…Îµ á¼”ÏÎ³Î¿Î½ á¼Ï€Î¿Î¯Ï‡ÎµÏƒÎ¸Î±Î¹Â· Î¼á¿¦Î¸Î¿Ï‚ Î´Ê¼ á¼„Î½Î´ÏÎµÏƒÏƒÎ¹ Î¼ÎµÎ»Î®ÏƒÎµÎ¹ Ï€á¾¶ÏƒÎ¹, Î¼Î¬Î»Î¹ÏƒÏ„Î± Î´Ê¼ á¼Î¼Î¿Î¯Â· Ï„Î¿á¿¦ Î³á½°Ï ÎºÏÎ¬Ï„Î¿Ï‚ á¼”ÏƒÏ„Ê¼ á¼Î½á½¶ Î¿á¼´Îºá¿³.\n",
      "\n",
      "Homer Odyssey 17.46-17.56\n",
      "Î¼á¿†Ï„ÎµÏ á¼Î¼Î®, Î¼Î® Î¼Î¿Î¹ Î³ÏŒÎ¿Î½ á½„ÏÎ½Ï…Î¸Î¹ Î¼Î·Î´Î­ Î¼Î¿Î¹ á¼¦Ï„Î¿Ï á¼Î½ ÏƒÏ„Î®Î¸ÎµÏƒÏƒÎ¹Î½ á½„ÏÎ¹Î½Îµ Ï†Ï…Î³ÏŒÎ½Ï„Î¹ Ï€ÎµÏ Î±á¼°Ï€á½ºÎ½ á½„Î»ÎµÎ¸ÏÎ¿Î½Â· á¼€Î»Î»Ê¼ á½‘Î´ÏÎ·Î½Î±Î¼Î­Î½Î·, ÎºÎ±Î¸Î±Ïá½° Ï‡ÏÎ¿á¿’ Îµá¼µÎ¼Î±Î¸Ê¼ á¼‘Î»Î¿á¿¦ÏƒÎ±, Îµá¼°Ï‚ á½‘Ï€ÎµÏá¿·Ê¼ á¼€Î½Î±Î²á¾¶ÏƒÎ± Ïƒá½ºÎ½ á¼€Î¼Ï†Î¹Ï€ÏŒÎ»Î¿Î¹ÏƒÎ¹ Î³Ï…Î½Î±Î¹Î¾á½¶Î½ Îµá½”Ï‡ÎµÎ¿ Ï€á¾¶ÏƒÎ¹ Î¸ÎµÎ¿á¿–ÏƒÎ¹ Ï„ÎµÎ»Î·Î­ÏƒÏƒÎ±Ï‚ á¼‘ÎºÎ±Ï„ÏŒÎ¼Î²Î±Ï‚ á¿¥Î­Î¾ÎµÎ¹Î½, Î±á¼´ ÎºÎ­ Ï€Î¿Î¸Î¹ Î–Îµá½ºÏ‚ á¼„Î½Ï„Î¹Ï„Î± á¼”ÏÎ³Î± Ï„ÎµÎ»Î­ÏƒÏƒá¿ƒ. Î±á½Ï„á½°Ï á¼Î³á½¼Î½ á¼€Î³Î¿Ïá½´Î½ á¼ÏƒÎµÎ»ÎµÏÏƒÎ¿Î¼Î±Î¹, á½„Ï†ÏÎ± ÎºÎ±Î»Î­ÏƒÏƒÏ‰ Î¾Îµá¿–Î½Î¿Î½, á½…Ï„Î¹Ï‚ Î¼Î¿Î¹ ÎºÎµá¿–Î¸ÎµÎ½ á¼…Î¼Ê¼ á¼•ÏƒÏ€ÎµÏ„Î¿ Î´Îµá¿¦ÏÎ¿ ÎºÎ¹ÏŒÎ½Ï„Î¹. Ï„á½¸Î½ Î¼á½²Î½ á¼Î³á½¼ Ï€ÏÎ¿á½”Ï€ÎµÎ¼ÏˆÎ± Ïƒá½ºÎ½ á¼€Î½Ï„Î¹Î¸Î­Î¿Î¹Ï‚ á¼‘Ï„Î¬ÏÎ¿Î¹ÏƒÎ¹, Î ÎµÎ¯ÏÎ±Î¹Î¿Î½ Î´Î­ Î¼Î¹Î½ á¼ Î½ÏÎ³ÎµÎ± Ï€ÏÎ¿Ï„á½¶ Î¿á¼¶ÎºÎ¿Î½ á¼„Î³Î¿Î½Ï„Î± á¼Î½Î´Ï…ÎºÎ­Ï‰Ï‚ Ï†Î¹Î»Î­ÎµÎ¹Î½ ÎºÎ±á½¶ Ï„Î¹Î­Î¼ÎµÎ½, Îµá¼°Ï‚ á½… ÎºÎµÎ½ á¼”Î»Î¸Ï‰.\n",
      "\n",
      "Homer Odyssey 17.108-17.149\n",
      "Ï„Î¿Î¹Î³á½°Ï á¼Î³Ï Ï„Î¿Î¹, Î¼á¿†Ï„ÎµÏ, á¼€Î»Î·Î¸ÎµÎ¯Î·Î½ ÎºÎ±Ï„Î±Î»Î­Î¾Ï‰. á¾ Ï‡ÏŒÎ¼ÎµÎ¸Ê¼ á¼”Ï‚ Ï„Îµ Î ÏÎ»Î¿Î½ ÎºÎ±á½¶ ÎÎ­ÏƒÏ„Î¿ÏÎ±, Ï€Î¿Î¹Î¼Î­Î½Î± Î»Î±á¿¶Î½Â· Î´ÎµÎ¾Î¬Î¼ÎµÎ½Î¿Ï‚ Î´Î­ Î¼Îµ ÎºÎµá¿–Î½Î¿Ï‚ á¼Î½ á½‘ÏˆÎ·Î»Î¿á¿–ÏƒÎ¹ Î´ÏŒÎ¼Î¿Î¹ÏƒÎ¹Î½ á¼Î½Î´Ï…ÎºÎ­Ï‰Ï‚ á¼Ï†Î¯Î»ÎµÎ¹, á½¡Ï‚ Îµá¼´ Ï„Îµ Ï€Î±Ï„á½´Ï á¼‘á½¸Î½ Ï…á¼±á½¸Î½ á¼Î»Î¸ÏŒÎ½Ï„Î± Ï‡ÏÏŒÎ½Î¹Î¿Î½ Î½Î­Î¿Î½ á¼„Î»Î»Î¿Î¸ÎµÎ½Â· á½£Ï‚ á¼Î¼á½² ÎºÎµá¿–Î½Î¿Ï‚ á¼Î½Î´Ï…ÎºÎ­Ï‰Ï‚ á¼ÎºÏŒÎ¼Î¹Î¶Îµ Ïƒá½ºÎ½ Ï…á¼±Î¬ÏƒÎ¹ ÎºÏ…Î´Î±Î»Î¯Î¼Î¿Î¹ÏƒÎ¹Î½. Î±á½Ï„á½°Ï á½ˆÎ´Ï…ÏƒÏƒá¿†Î¿Ï‚ Ï„Î±Î»Î±ÏƒÎ¯Ï†ÏÎ¿Î½Î¿Ï‚ Î¿á½” Ï€Î¿Ï„Ê¼ á¼”Ï†Î±ÏƒÎºÎµÎ½, Î¶Ï‰Î¿á¿¦ Î¿á½Î´á½² Î¸Î±Î½ÏŒÎ½Ï„Î¿Ï‚, á¼Ï€Î¹Ï‡Î¸Î¿Î½Î¯Ï‰Î½ Ï„ÎµÏ… á¼€ÎºÎ¿á¿¦ÏƒÎ±Î¹Â· á¼€Î»Î»Î¬ Î¼Ê¼ á¼Ï‚ á¼ˆÏ„ÏÎµÎÎ´Î·Î½, Î´Î¿Ï…ÏÎ¹ÎºÎ»ÎµÎ¹Ï„á½¸Î½ ÎœÎµÎ½Î­Î»Î±Î¿Î½, á¼µÏ€Ï€Î¿Î¹ÏƒÎ¹ Ï€ÏÎ¿á½”Ï€ÎµÎ¼ÏˆÎµ ÎºÎ±á½¶ á¼…ÏÎ¼Î±ÏƒÎ¹ ÎºÎ¿Î»Î»Î·Ï„Î¿á¿–ÏƒÎ¹Î½. á¼”Î½Î¸Ê¼ á¼´Î´Î¿Î½ á¼ˆÏÎ³ÎµÎ¯Î·Î½ á¼™Î»Î­Î½Î·Î½, á¼§Ï‚ Îµá¼µÎ½ÎµÎºÎ± Ï€Î¿Î»Î»á½° á¼ˆÏÎ³Îµá¿–Î¿Î¹ Î¤Ïá¿¶Î­Ï‚ Ï„Îµ Î¸Îµá¿¶Î½ á¼°ÏŒÏ„Î·Ï„Î¹ Î¼ÏŒÎ³Î·ÏƒÎ±Î½. Îµá¼´ÏÎµÏ„Î¿ Î´Ê¼ Î±á½Ï„Î¯ÎºÊ¼ á¼”Ï€ÎµÎ¹Ï„Î± Î²Î¿á½´Î½ á¼€Î³Î±Î¸á½¸Ï‚ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚ á½…Ï„Ï„ÎµÏ… Ï‡ÏÎ·ÎÎ¶Ï‰Î½ á¼±ÎºÏŒÎ¼Î·Î½ Î›Î±ÎºÎµÎ´Î±Î¯Î¼Î¿Î½Î± Î´á¿–Î±Î½Â· Î±á½Ï„á½°Ï á¼Î³á½¼ Ï„á¿· Ï€á¾¶ÏƒÎ±Î½ á¼€Î»Î·Î¸ÎµÎ¯Î·Î½ ÎºÎ±Ï„Î­Î»ÎµÎ¾Î±Â· ÎºÎ±á½¶ Ï„ÏŒÏ„Îµ Î´Î® Î¼Îµ á¼”Ï€ÎµÏƒÏƒÎ¹Î½ á¼€Î¼ÎµÎ¹Î²ÏŒÎ¼ÎµÎ½Î¿Ï‚ Ï€ÏÎ¿ÏƒÎ­ÎµÎ¹Ï€ÎµÎ½Â· á½¢ Ï€ÏŒÏ€Î¿Î¹, á¼¦ Î¼Î¬Î»Î± Î´á½´ ÎºÏÎ±Ï„ÎµÏÏŒÏ†ÏÎ¿Î½Î¿Ï‚ á¼€Î½Î´Ïá½¸Ï‚ á¼Î½\n",
      " Îµá½Î½á¿‡ á¼¤Î¸ÎµÎ»Î¿Î½ Îµá½Î½Î·Î¸á¿†Î½Î±Î¹, á¼€Î½Î¬Î»ÎºÎ¹Î´ÎµÏ‚ Î±á½Ï„Î¿á½¶ á¼ÏŒÎ½Ï„ÎµÏ‚. á½¡Ï‚ Î´Ê¼ á½Ï€ÏŒÏ„Ê¼ á¼Î½ Î¾Ï…Î»ÏŒÏ‡á¿³ á¼”Î»Î±Ï†Î¿Ï‚ ÎºÏÎ±Ï„ÎµÏÎ¿á¿–Î¿ Î»Î­Î¿Î½Ï„Î¿Ï‚ Î½ÎµÎ²ÏÎ¿á½ºÏ‚ ÎºÎ¿Î¹Î¼Î®ÏƒÎ±ÏƒÎ± Î½ÎµÎ·Î³ÎµÎ½Î­Î±Ï‚ Î³Î±Î»Î±Î¸Î·Î½Î¿á½ºÏ‚ ÎºÎ½Î·Î¼Î¿á½ºÏ‚ á¼Î¾ÎµÏÎ­á¿ƒÏƒÎ¹ ÎºÎ±á½¶ á¼„Î³ÎºÎµÎ± Ï€Î¿Î¹Î®ÎµÎ½Ï„Î± Î²Î¿ÏƒÎºÎ¿Î¼Î­Î½Î·, á½ Î´Ê¼ á¼”Ï€ÎµÎ¹Ï„Î± á¼‘á½´Î½ Îµá¼°ÏƒÎ®Î»Ï…Î¸ÎµÎ½ Îµá½Î½Î®Î½, á¼€Î¼Ï†Î¿Ï„Î­ÏÎ¿Î¹ÏƒÎ¹ Î´á½² Ï„Î¿á¿–ÏƒÎ¹Î½ á¼€ÎµÎ¹ÎºÎ­Î± Ï€ÏŒÏ„Î¼Î¿Î½ á¼Ï†á¿†ÎºÎµÎ½, á½£Ï‚ á½ˆÎ´Ï…ÏƒÎµá½ºÏ‚ ÎºÎµÎ¯Î½Î¿Î¹ÏƒÎ¹Î½ á¼€ÎµÎ¹ÎºÎ­Î± Ï€ÏŒÏ„Î¼Î¿Î½ á¼Ï†Î®ÏƒÎµÎ¹. Î±á¼² Î³Î¬Ï, Î–Îµá¿¦ Ï„Îµ Ï€Î¬Ï„ÎµÏ ÎºÎ±á½¶ á¼ˆÎ¸Î·Î½Î±Î¯Î· ÎºÎ±á½¶ á¼ŒÏ€Î¿Î»Î»Î¿Î½, Ï„Î¿á¿–Î¿Ï‚ á¼á½¼Î½ Î¿á¼·ÏŒÏ‚ Ï€Î¿Ï„Ê¼ á¼Ï‹ÎºÏ„Î¹Î¼Î­Î½á¿ƒ á¼Î½á½¶ Î›Î­ÏƒÎ²á¿³ á¼Î¾ á¼”ÏÎ¹Î´Î¿Ï‚ Î¦Î¹Î»Î¿Î¼Î·Î»ÎµÎÎ´á¿ƒ á¼Ï€Î¬Î»Î±Î¹ÏƒÎµÎ½ á¼€Î½Î±ÏƒÏ„Î¬Ï‚, Îºá½°Î´ Î´Ê¼ á¼”Î²Î±Î»Îµ ÎºÏÎ±Ï„ÎµÏá¿¶Ï‚, ÎºÎµÏ‡Î¬ÏÎ¿Î½Ï„Î¿ Î´á½² Ï€Î¬Î½Ï„ÎµÏ‚ á¼ˆÏ‡Î±Î¹Î¿Î¯, Ï„Î¿á¿–Î¿Ï‚ á¼á½¼Î½ Î¼Î½Î·ÏƒÏ„á¿†ÏÏƒÎ¹Î½ á½Î¼Î¹Î»Î®ÏƒÎµÎ¹ÎµÎ½ á½ˆÎ´Ï…ÏƒÏƒÎµÏÏ‚Â· Ï€Î¬Î½Ï„ÎµÏ‚ ÎºÊ¼ á½ ÎºÏÎ¼Î¿ÏÎ¿Î¯ Ï„Îµ Î³ÎµÎ½Î¿Î¯Î±Ï„Î¿ Ï€Î¹ÎºÏÏŒÎ³Î±Î¼Î¿Î¯ Ï„Îµ. Ï„Î±á¿¦Ï„Î± Î´Ê¼ á¼… Î¼Ê¼ Îµá¼°ÏÏ‰Ï„á¾·Ï‚ ÎºÎ±á½¶ Î»Î¯ÏƒÏƒÎµÎ±Î¹, Î¿á½Îº á¼‚Î½ á¼Î³Ï Î³Îµ á¼„Î»Î»Î± Ï€Î±Ïá½²Î¾ Îµá¼´Ï€Î¿Î¹Î¼Î¹ Ï€Î±ÏÎ±ÎºÎ»Î¹Î´á½¸Î½ Î¿á½Î´Ê¼ á¼€Ï€Î±Ï„Î®ÏƒÏ‰, á¼€Î»Î»á½° Ï„á½° Î¼Î­Î½ Î¼Î¿Î¹ á¼”ÎµÎ¹Ï€Îµ Î³Î­ÏÏ‰Î½ á¼…Î»Î¹Î¿Ï‚ Î½Î·Î¼ÎµÏÏ„Î®Ï‚, Ï„á¿¶Î½ Î¿á½Î´Î­Î½ Ï„Î¿Î¹ á¼Î³á½¼ ÎºÏÏÏˆÏ‰ á¼”Ï€Î¿Ï‚ Î¿á½Î´Ê¼ á¼Ï€Î¹ÎºÎµÏÏƒÏ‰. Ï†á¿† Î¼Î¹Î½ á½… Î³Ê¼ á¼Î½ Î½Î®Ïƒá¿³ á¼°Î´Î­ÎµÎ¹Î½ ÎºÏÎ±Ï„Î­ÏÊ¼ á¼„Î»Î³ÎµÊ¼ á¼”Ï‡Î¿Î½Ï„Î±, Î½ÏÎ¼Ï†Î·Ï‚ á¼Î½ Î¼ÎµÎ³Î¬ÏÎ¿Î¹ÏƒÎ¹ ÎšÎ±Î»Ï…ÏˆÎ¿á¿¦Ï‚, á¼¥ Î¼Î¹Î½ á¼€Î½Î¬Î³Îºá¿ƒ á¼´ÏƒÏ‡ÎµÎ¹Â· á½ Î´Ê¼ Î¿á½ Î´ÏÎ½Î±Ï„Î±Î¹ á¼£Î½ Ï€Î±Ï„ÏÎ¯Î´Î± Î³Î±á¿–Î±Î½ á¼±ÎºÎ­ÏƒÎ¸Î±Î¹. Î¿á½ Î³Î¬Ï Î¿á¼± Ï€Î¬ÏÎ± Î½á¿†ÎµÏ‚ á¼Ï€Î®ÏÎµÏ„Î¼Î¿Î¹ ÎºÎ±á½¶ á¼‘Ï„Î±á¿–ÏÎ¿Î¹, Î¿á¼µ ÎºÎ­Î½ Î¼Î¹Î½ Ï€Î­Î¼Ï€Î¿Î¹ÎµÎ½ á¼Ï€Ê¼ Îµá½ÏÎ­Î± Î½á¿¶Ï„Î± Î¸Î±Î»Î¬ÏƒÏƒÎ·Ï‚ á½£Ï‚ á¼”Ï†Î±Ï„Ê¼ á¼ˆÏ„ÏÎµÎÎ´Î·Ï‚, Î´Î¿Ï…ÏÎ¹ÎºÎ»ÎµÎ¹Ï„á½¸Ï‚ ÎœÎµÎ½Î­Î»Î±Î¿Ï‚. Ï„Î±á¿¦Ï„Î± Ï„ÎµÎ»ÎµÏ…Ï„Î®ÏƒÎ±Ï‚ Î½ÎµÏŒÎ¼Î·Î½Â· á¼”Î´Î¿ÏƒÎ±Î½ Î´Î­ Î¼Î¿Î¹ Î¿á½–ÏÎ¿Î½ á¼€Î¸Î¬Î½Î±Ï„Î¿Î¹, Ï„Î¿Î¯ Î¼Ê¼ á½¦ÎºÎ± Ï†Î¯Î»Î·Î½ á¼Ï‚ Ï€Î±Ï„ÏÎ¯Î´Ê¼ á¼”Ï€ÎµÎ¼ÏˆÎ±Î½.\n",
      "\n",
      "Homer Odyssey 18.227-18.242\n",
      "Î¼á¿†Ï„ÎµÏ á¼Î¼Î®, Ï„á½¸ Î¼á½²Î½ Î¿á½” ÏƒÎµ Î½ÎµÎ¼ÎµÏƒÏƒá¿¶Î¼Î±Î¹ ÎºÎµÏ‡Î¿Î»á¿¶ÏƒÎ¸Î±Î¹Â· Î±á½Ï„á½°Ï á¼Î³á½¼ Î¸Ï…Î¼á¿· Î½Î¿Î­Ï‰ ÎºÎ±á½¶ Î¿á¼¶Î´Î± á¼•ÎºÎ±ÏƒÏ„Î±, á¼ÏƒÎ¸Î»Î¬ Ï„Îµ ÎºÎ±á½¶ Ï„á½° Ï‡Î­ÏÎµÎ¹Î±Â· Ï€Î¬ÏÎ¿Ï‚ Î´Ê¼ á¼”Ï„Î¹ Î½Î®Ï€Î¹Î¿Ï‚ á¼¦Î±. á¼€Î»Î»Î¬ Ï„Î¿Î¹ Î¿á½ Î´ÏÎ½Î±Î¼Î±Î¹ Ï€ÎµÏ€Î½Ï…Î¼Î­Î½Î± Ï€Î¬Î½Ï„Î± Î½Î¿á¿†ÏƒÎ±Î¹Â· á¼Îº Î³Î¬Ï Î¼Îµ Ï€Î»Î®ÏƒÏƒÎ¿Ï…ÏƒÎ¹ Ï€Î±ÏÎ®Î¼ÎµÎ½Î¿Î¹ á¼„Î»Î»Î¿Î¸ÎµÎ½ á¼„Î»Î»Î¿Ï‚ Î¿á¼µÎ´Îµ ÎºÎ±Îºá½° Ï†ÏÎ¿Î½Î­Î¿Î½Ï„ÎµÏ‚, á¼Î¼Î¿á½¶ Î´Ê¼ Î¿á½Îº Îµá¼°Ïƒá½¶Î½ á¼€ÏÏ‰Î³Î¿Î¯. Î¿á½ Î¼Î­Î½ Ï„Î¿Î¹ Î¾ÎµÎ¯Î½Î¿Ï… Î³Îµ ÎºÎ±á½¶ á¼¼ÏÎ¿Ï… Î¼á¿¶Î»Î¿Ï‚ á¼Ï„ÏÏ‡Î¸Î· Î¼Î½Î·ÏƒÏ„Î®ÏÏ‰Î½ á¼°ÏŒÏ„Î·Ï„Î¹, Î²Î¯á¿ƒ Î´Ê¼ á½… Î³Îµ Ï†Î­ÏÏ„ÎµÏÎ¿Ï‚ á¼¦ÎµÎ½. Î±á¼² Î³Î¬Ï, Î–Îµá¿¦ Ï„Îµ Ï€Î¬Ï„ÎµÏ ÎºÎ±á½¶ á¼ˆÎ¸Î·Î½Î±Î¯Î· ÎºÎ±á½¶ á¼ŒÏ€Î¿Î»Î»Î¿Î½, Î¿á½•Ï„Ï‰ Î½á¿¦Î½ Î¼Î½Î·ÏƒÏ„á¿†ÏÎµÏ‚ á¼Î½ á¼¡Î¼ÎµÏ„Î­ÏÎ¿Î¹ÏƒÎ¹ Î´ÏŒÎ¼Î¿Î¹ÏƒÎ¹ Î½ÎµÏÎ¿Î¹ÎµÎ½ ÎºÎµÏ†Î±Î»á½°Ï‚ Î´ÎµÎ´Î¼Î·Î¼Î­Î½Î¿Î¹, Î¿á¼± Î¼á½²Î½ á¼Î½ Î±á½Î»á¿‡, Î¿á¼± Î´Ê¼ á¼”Î½Ï„Î¿ÏƒÎ¸Îµ Î´ÏŒÎ¼Î¿Î¹Î¿, Î»ÎµÎ»á¿¦Ï„Î¿ Î´á½² Î³Ï…á¿–Î± á¼‘ÎºÎ¬ÏƒÏ„Î¿Ï…, á½¡Ï‚ Î½á¿¦Î½ á¼¾ÏÎ¿Ï‚ ÎºÎµá¿–Î½Î¿Ï‚ á¼Ï€Ê¼ Î±á½Î»ÎµÎ¯á¿ƒÏƒÎ¹ Î¸ÏÏá¿ƒÏƒÎ¹Î½ á¼§ÏƒÏ„Î±Î¹ Î½ÎµÏ…ÏƒÏ„Î¬Î¶Ï‰Î½ ÎºÎµÏ†Î±Î»á¿‡, Î¼ÎµÎ¸ÏÎ¿Î½Ï„Î¹ á¼Î¿Î¹ÎºÏÏ‚, Î¿á½Î´Ê¼ á½€ÏÎ¸á½¸Ï‚ ÏƒÏ„á¿†Î½Î±Î¹ Î´ÏÎ½Î±Ï„Î±Î¹ Ï€Î¿Ïƒá½¶Î½ Î¿á½Î´á½² Î½Î­ÎµÏƒÎ¸Î±Î¹ Î¿á¼´ÎºÎ±Î´Ê¼, á½…Ï€Î· Î¿á¼± Î½ÏŒÏƒÏ„Î¿Ï‚, á¼Ï€Îµá½¶ Ï†Î¯Î»Î± Î³Ï…á¿–Î± Î»Î­Î»Ï…Î½Ï„Î±Î¹.\n",
      "\n",
      "Homer Odyssey 21.344-21.353\n",
      "Î¼á¿†Ï„ÎµÏ á¼Î¼Î®, Ï„ÏŒÎ¾Î¿Î½ Î¼á½²Î½ á¼ˆÏ‡Î±Î¹á¿¶Î½ Î¿á½” Ï„Î¹Ï‚ á¼Î¼Îµá¿–Î¿ ÎºÏÎµÎ¯ÏƒÏƒÏ‰Î½, á¾§ ÎºÊ¼ á¼Î¸Î­Î»Ï‰, Î´ÏŒÎ¼ÎµÎ½Î±Î¯ Ï„Îµ ÎºÎ±á½¶ á¼€ÏÎ½Î®ÏƒÎ±ÏƒÎ¸Î±Î¹, Î¿á½”Î¸Ê¼ á½…ÏƒÏƒÎ¿Î¹ ÎºÏÎ±Î½Î±á½´Î½ á¼¸Î¸Î¬ÎºÎ·Î½ ÎºÎ¬Ï„Î± ÎºÎ¿Î¹ÏÎ±Î½Î­Î¿Ï…ÏƒÎ¹Î½, Î¿á½”Î¸Ê¼ á½…ÏƒÏƒÎ¿Î¹ Î½Î®ÏƒÎ¿Î¹ÏƒÎ¹ Ï€Ïá½¸Ï‚ á¼¬Î»Î¹Î´Î¿Ï‚ á¼±Ï€Ï€Î¿Î²ÏŒÏ„Î¿Î¹Î¿Â· Ï„á¿¶Î½ Î¿á½” Ï„Î¯Ï‚ Î¼Ê¼ á¼€Î­ÎºÎ¿Î½Ï„Î± Î²Î¹Î®ÏƒÎµÏ„Î±Î¹, Î±á¼´ ÎºÊ¼ á¼Î¸Î­Î»Ï‰Î¼Î¹ ÎºÎ±á½¶ ÎºÎ±Î¸Î¬Ï€Î±Î¾ Î¾ÎµÎ¯Î½á¿³ Î´ÏŒÎ¼ÎµÎ½Î±Î¹ Ï„Î¬Î´Îµ Ï„ÏŒÎ¾Î± Ï†Î­ÏÎµÏƒÎ¸Î±Î¹. á¼€Î»Î»Ê¼ Îµá¼°Ï‚ Î¿á¼¶ÎºÎ¿Î½ á¼°Î¿á¿¦ÏƒÎ± Ï„á½° ÏƒÊ¼ Î±á½Ï„á¿†Ï‚ á¼”ÏÎ³Î± ÎºÏŒÎ¼Î¹Î¶Îµ, á¼±ÏƒÏ„ÏŒÎ½ Ï„Ê¼ á¼ Î»Î±ÎºÎ¬Ï„Î·Î½ Ï„Îµ, ÎºÎ±á½¶ á¼€Î¼Ï†Î¹Ï€ÏŒÎ»Î¿Î¹ÏƒÎ¹ ÎºÎ­Î»ÎµÏ…Îµ á¼”ÏÎ³Î¿Î½ á¼Ï€Î¿Î¯Ï‡ÎµÏƒÎ¸Î±Î¹Â· Ï„ÏŒÎ¾Î¿Î½ Î´Ê¼ á¼„Î½Î´ÏÎµÏƒÏƒÎ¹ Î¼ÎµÎ»Î®ÏƒÎµÎ¹ Ï€á¾¶ÏƒÎ¹, Î¼Î¬Î»Î¹ÏƒÏ„Î± Î´Ê¼ á¼Î¼Î¿Î¯Â· Ï„Î¿á¿¦ Î³á½°Ï ÎºÏÎ¬Ï„Î¿Ï‚ á¼”ÏƒÏ„Ê¼ á¼Î½á½¶ Î¿á¼´Îºá¿³.\n",
      "\n",
      "Homer Odyssey 23.97-23.103\n",
      "Î¼á¿†Ï„ÎµÏ á¼Î¼Î®, Î´ÏÏƒÎ¼Î·Ï„ÎµÏ, á¼€Ï€Î·Î½Î­Î± Î¸Ï…Î¼á½¸Î½ á¼”Ï‡Î¿Ï…ÏƒÎ±, Ï„Î¯Ï†Î¸Ê¼ Î¿á½•Ï„Ï‰ Ï€Î±Ï„Ïá½¸Ï‚ Î½Î¿ÏƒÏ†Î¯Î¶ÎµÎ±Î¹, Î¿á½Î´á½² Ï€Î±ÏÊ¼ Î±á½Ï„á½¸Î½ á¼‘Î¶Î¿Î¼Î­Î½Î· Î¼ÏÎ¸Î¿Î¹ÏƒÎ¹Î½ á¼€Î½ÎµÎ¯ÏÎµÎ±Î¹ Î¿á½Î´á½² Î¼ÎµÏ„Î±Î»Î»á¾·Ï‚; Î¿á½ Î¼Î­Î½ ÎºÊ¼ á¼„Î»Î»Î· Î³Ê¼ á½§Î´Îµ Î³Ï…Î½á½´ Ï„ÎµÏ„Î»Î·ÏŒÏ„Î¹ Î¸Ï…Î¼á¿· á¼€Î½Î´Ïá½¸Ï‚ á¼€Ï†ÎµÏƒÏ„Î±Î¯Î·, á½…Ï‚ Î¿á¼± ÎºÎ±Îºá½° Ï€Î¿Î»Î»á½° Î¼Î¿Î³Î®ÏƒÎ±Ï‚ á¼”Î»Î¸Î¿Î¹ á¼ÎµÎ¹ÎºÎ¿ÏƒÏ„á¿· á¼”Ï„ÎµÏŠ á¼Ï‚ Ï€Î±Ï„ÏÎ¯Î´Î± Î³Î±á¿–Î±Î½Â· ÏƒÎ¿á½¶ Î´Ê¼ Î±á¼°Îµá½¶ ÎºÏÎ±Î´Î¯Î· ÏƒÏ„ÎµÏÎµÏ‰Ï„Î­ÏÎ· á¼ÏƒÏ„á½¶ Î»Î¯Î¸Î¿Î¹Î¿.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passages = []\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    text = cts_passage.text\n",
    "    passages.append(text)\n",
    "    \n",
    "    print(f'{s.author.name} {s.work.title} {s.l_range}')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CLTK to parse the text\n",
    "\n",
    "We can use CTLK's tokenizers to break each string into meaningful units -- sentences and/or words. Then we use the backoff lemmatizer to normalize all the inflected forms to dictionary headwords.\n",
    "\n",
    "I rolled these steps into one convenience function up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feat_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_k/nhmmjzg96r318bm9jvyvv7ph0000gn/T/ipykernel_58561/205355683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpassages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcltk_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcltk_nlp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     these_lems = [lem for tok, lem in lemmatized]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/nlp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/nlp.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0ma_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_process_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/processes.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_doc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moutput_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mstanza_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/boltons/cacheutils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/processes.py\u001b[0m in \u001b[0;36malgorithm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcachedproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mStanzaWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_doc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36mget_nlp\u001b[0;34m(cls, language, treebank)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, treebank, stanza_debug_level, interactive, silent)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# for the log file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/cltk/dependency/stanza.py\u001b[0m in \u001b[0;36m_load_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mlogging_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanza_debug_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# default, won't fail if GPU not present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mlemma_use_identity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemma_use_identity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         )\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[1;32m    142\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m# if there was a requirements issue, add it to list which will be printed at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_variant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# build the final config for the processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pre_tokenized_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# load everything from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# build model from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/dices-examples/venv/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# were built with mwt layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_mwt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emb_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat_dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feat_dropout'"
     ]
    }
   ],
   "source": [
    "lems = Counter()\n",
    "for p in passages:\n",
    "    lang = s.work.lang\n",
    "    cltk_doc = cltk_nlp[lang](p)\n",
    "    \n",
    "#     these_lems = [lem for tok, lem in lemmatized]\n",
    "#     lems.update(these_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the counter to a Pandas data frame for tidier presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lems.most_common(), columns=['lemma', 'count'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now let's think more broadly. How typical is this kind of speech? We can use external linked data to find other examples of mother-son conversations in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom code to query WikiData\n",
    "\n",
    "This lets us ask whether a given addressee belongs to the set of people having a certain relationship to a given speaker. It takes a while to download the WikiData entities, and I had to run this a number of times, so I cached WD data in the respective character objects once it's downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWD(c):\n",
    "    '''make sure character has wikidata id'''\n",
    "    if c.char is not None:\n",
    "        if c.char.wd is not None:\n",
    "            if len(c.char.wd.strip()) > 0:\n",
    "                return c.char.wd.strip()\n",
    "\n",
    "def checkWDRelation(s, a, relation, cache=None):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    else:\n",
    "        if (s.id, a.id) in cache:\n",
    "            return cache[(s.id, a.id)]\n",
    "\n",
    "    res = False\n",
    "\n",
    "    if not hasattr(s, 'wd_ent'):\n",
    "        s.wd_ent = WikidataItem(get_entity_dict_from_api(s.wd))\n",
    "\n",
    "    claim_group = s.wd_ent.get_truthy_claim_group(relation)\n",
    "\n",
    "    for claim in claim_group:\n",
    "        if claim.mainsnak.datavalue is None:\n",
    "            continue\n",
    "        if claim.mainsnak.datavalue.value['id'] == a.wd:\n",
    "            res = True\n",
    "    \n",
    "    cache[(s.id, a.id)] = res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the relation \"mother of\" has the WikiData ID `'P25'`. Here's how we ask if a given addressee is the mother of a given speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Telemachus')[0]\n",
    "addressee = api.getCharacters(name='Penelope')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added a separate cache just for the boolean result of checkWDRelation, to save a little more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_mothers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WikiData to filter the speeches\n",
    "\n",
    "The DICES dataset includes WikiData ids for most of the characters (not all). The DICES API doesn't let us query WikiData itself, though. For now, the easiest thing is just to download all the speeches and character IDs, and then cross reference them against WikiData using its own API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the speeches: takes a minute\n",
    "speeches = (\n",
    "    api.getSpeeches(author_name='Homer', progress=True) +\n",
    "    api.getSpeeches(author_name='Apollonius', progress=True) +\n",
    "    api.getSpeeches(author_name='Virgil', progress=True))\n",
    "\n",
    "speeches.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check each speaker-addressee pair against WikiData**\n",
    "\n",
    "What we actually do here is download the WikiData entity for each speaker, if we don't already have it cached. Then we ask the WD entity for its mom(s), and check the WD ID of the addressee against the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty table\n",
    "rows = []\n",
    "\n",
    "# create a progress bar\n",
    "pbar = NotebookPBar(start=0, max=len(speeches))\n",
    "\n",
    "# iterate over all the speeches, checking each speaker-addressee combination\n",
    "for s in speeches:\n",
    "    if s.spkr is not None and s.addr is not None:\n",
    "        for spkr in s.spkr:\n",
    "            spkr_wd = checkWD(spkr)\n",
    "            if spkr_wd is not None:\n",
    "\n",
    "                for addr in s.addr:\n",
    "                    addr_wd = checkWD(addr)\n",
    "                    if addr_wd is not None:\n",
    "                        rows.append((\n",
    "                            s.id,\n",
    "                            s.work.title,\n",
    "                            s.l_fi,\n",
    "                            s.l_la,\n",
    "                            spkr.char.name, spkr_wd, \n",
    "                            addr.char.name, addr_wd,\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P25', cache=cache_mothers),\n",
    "                            checkWDRelation(addr.char, spkr.char, 'P25', cache=cache_mothers)\n",
    "                            ))\n",
    "    pbar.update()\n",
    "\n",
    "# finally, organize the table as a pandas data frame\n",
    "df = pd.DataFrame(rows, columns=['id', 'work', 'l_first', 'l_last', 'spkr', 'sp_wd', 'addr', 'ad_wd', 'sp_is_mom', 'ad_is_mom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤” Let's take a look at the results. Here is the complete set of speeches, with the additional attribute `sp_is_mom` if the speaker is the addressee's mother, and `ad_is_mom` if the addressee is the speaker's mother.\n",
    "\n",
    "As a quick sanity check, the first two speeches in the Argonautica, which were at the top of the list when I ran this, are between Jason and his mother, Alcimede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['work']=='Argonautica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to pandas, we can filter the data frame on the new boolean columns to show only speeches between mother and child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = df.loc[df['sp_is_mom'] | df['ad_is_mom'],\n",
    "             ['work', 'l_first', 'l_last', 'spkr', 'addr']]\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also comes in handy if I wanted to export this data to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Let's see how well the automated approach worked. We'll load up a hand-corrected list of mother-child speeches and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('data/moms-bench.csv', dtype=str)\n",
    "bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the union of `hits` and `bench` to see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hits.merge(bench, on=['work', 'l_first'], how='outer', \n",
    "                        suffixes=['_h', '_b'], indicator=True)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(results[['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b', '_merge']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = sum(results['_merge'] == 'both')\n",
    "\n",
    "p = true_pos / hits.shape[0]\n",
    "r = true_pos / bench.shape[0]\n",
    "\n",
    "print(f'Precision: {p:.2f}')\n",
    "print(f'Recall:    {r:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### The good news\n",
    "We got almost all of the benchmark set, with exceptions to be discussed below, and no false positives.\n",
    "\n",
    "#### The bad news\n",
    "Let's look a little more closely at the speeches we missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = results[results['_merge'] == 'right_only'][\n",
    "                ['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b']]\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, I'd say these fall into three groups:\n",
    "\n",
    " 1. A conversation in the Iliad between Hera and a group of gods, some of whom were here children\n",
    " 2. A conversation in the Aeneid between Jupiter and \"Cybele,\" i.e., Rhea.\n",
    " 3. A conversation in the Aeneid between Euryalus and his anonymous mother.\n",
    "\n",
    "#### Digging a little deeper\n",
    "\n",
    "First, let's confirm that all these speeches are in the database results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed.merge(df, how='left', on=['work', 'l_first'])[[\n",
    "    'work', 'l_first',                               # keys: work and locus\n",
    "    'id', 'spkr', 'addr', 'sp_is_mom', 'ad_is_mom',  # cols from df\n",
    "    'spkr_b', 'addr_b'                               # cols from bench\n",
    "    \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech between Euyalus and his mom is missing from the database. That's because as of this writing we don't have a systematic way of including anonymous characters like her--folks described only by a family relation or an occupation. Because the character doesn't fit our current data model, she gets omitted, and this speech fails to be added to the database.\n",
    "\n",
    "The conversation between Hera and the gods is there in the database, but the speaker-addressee pairs are not registering as mother-child relationships. In this case, it's because \"gods\" isn't being parsed as including all the individual gods, but rather a corporate entity that doesn't have \"mother\" or \"child\" as properties. This highlights another issue that needs to be resolved in our data model.\n",
    "\n",
    "Finally, the conversation between Jupiter and his mom is also in the database, and each of these characters is matched with a WikiData entity, but we're not getting the right answer about their kinship relation because WikiData has distinct entities for the Greek goddess Rhea and the Phrygian goddess Cybele. We can fix this by pointing the character's WikiData ID to the Greek goddess instead (as we did for the Roman deities), but maybe we should think about the larger problem of poetic ambiguity/metonymy/syncretism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    " - WikiData gave us a lot for free -- all of the individual mother-child relationships were in there when we knew where to look.\n",
    " \n",
    " - There is still some important work to be done refining our underlying data model.\n",
    " \n",
    " - If we want to rely on linked open data for high-stakes work, we need resources that are sensitive to the details we care about. We hope that MANTO, because it's specific to Classical myth and hand-curated by domain experts, can help us with problems like when to treat Cybele and Rhea as independent entities and when to consider them identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
