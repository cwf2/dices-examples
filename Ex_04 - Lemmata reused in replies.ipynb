{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing with CLTK\n",
    "\n",
    "Let's investigate to what degree speeches that come second in a conversation re-use the language of the speech to which they reply.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### A couple of useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DICES API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "api = DicesAPI(progress_class=NotebookPBar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK setup\n",
    "\n",
    "If you haven't used CLTK before, you may need to download models and texts before some functions will work. Normally, you only need to run this once. If you're working on Binder, though, you start from a clean system every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "print('Downloading models:')\n",
    "\n",
    "for lang in ['latin', 'greek']:\n",
    "    print(' - ' + lang)\n",
    "    downloader = CorpusImporter(lang)\n",
    "    downloader.import_corpus(f'{lang}_models_cltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers and Lemmatizers\n",
    "\n",
    "CLTK uses language-specific tokenizers and lemmatizers. I like to have one convenience function that I can call on every speech, regardless of language. That means I have to set up language-specific tokenizers and lemmatizers first, and also cook up some kludgey regular expression substitutions to normalize orthography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.lemmatize.greek.backoff import BackoffGreekLemmatizer\n",
    "\n",
    "# language-specific tokenizers\n",
    "tokenizer = {\n",
    "    'greek': WordTokenizer('greek'),\n",
    "    'latin': WordTokenizer('latin'),\n",
    "}\n",
    "\n",
    "# language-specific lemmatizers\n",
    "lemmatizer = {\n",
    "    'greek': BackoffGreekLemmatizer(),\n",
    "    'latin': BackoffLatinLemmatizer(),    \n",
    "}\n",
    "\n",
    "# regular expressions to tidy up perseus texts for ctlk\n",
    "replacements = {\n",
    "    'greek': [\n",
    "        (r'Â·', ','),           # FIXME: raised dot? \n",
    "        (chr(700), chr(8217)), # two different apostrophes that look alike\n",
    "    ],\n",
    "    'latin': [\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "# compile the regexes\n",
    "for lang in ['greek', 'latin']:\n",
    "    replacements[lang] = [(re.compile(pat), repl) for pat, repl in replacements[lang]]\n",
    "\n",
    "# wrap everything in a generic tokenize-lemmatize function\n",
    "def lemmatize(text, lang):\n",
    "    '''return a set of (token,lemmata) pairs for a string'''\n",
    "    \n",
    "    for pat, repl in replacements[lang]:\n",
    "        text = pat.sub(repl, text)\n",
    "    \n",
    "    tokens = tokenizer[lang].tokenize(text)\n",
    "    lemmata = lemmatizer[lang].lemmatize(tokens)\n",
    "    \n",
    "    return lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the speeches\n",
    "\n",
    "### Query the DICES API\n",
    "\n",
    "For the moment, at least, it's generally easier to download an inclusive set of speeches from the remote server all at once, then filter them locally using the client library. Here, we download all speeches in Homer. \n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"margin: 1em 2em\">\n",
    "    <p>We could have used <code>author_name='Homer'</code> as the sole search param, but this way we can showcase concatenation of results with the <code>+</code> operator.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(work_title='Iliad', progress=True) + \\\n",
    "            api.getSpeeches(work_title='Odyssey', progress=True)\n",
    "speeches.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the text of the speeches\n",
    "\n",
    "Before we can do any NLP, we have to get the text of the speeches from the remote library. In this loop, we download the CTS passage for each speech in turn, appending the plain text of the passage to the respective speech object as a new attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar\n",
    "pbar = NotebookPBar(start=0, max=len(speeches))\n",
    "\n",
    "# download text, add to speech object as new attribute\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    s.text = cts_passage.text\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the speech text with CLTK\n",
    "\n",
    "Now we can run CLTK's tokenizers and lemmatizers, using the wrapper function defined above. The lemmatizer returns two lists, one of tokens and one of lemmata in their dictionary form. I'm saving each of these lists to the original speech object as a new attribute, just to make sure I don't lose track of which lemmata go with which speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progress bar\n",
    "pbar = NotebookPBar(start=0, max=len(speeches))\n",
    "\n",
    "# iterate over speeches\n",
    "for s in speeches:\n",
    "    lang = s.work.lang\n",
    "    toks = []\n",
    "    lems = []\n",
    "    \n",
    "    # lemmatizer delivers two lists, one of tokens and one of lemmata\n",
    "    for t, l in lemmatize(s.text.lower(), lang):\n",
    "        toks.append(t)\n",
    "        lems.append(l)\n",
    "        \n",
    "    # append toks, lems, to speech object as new attributes\n",
    "    s.tokens = toks\n",
    "    s.lemmata = lems\n",
    "        \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for shared lemmata\n",
    "\n",
    "Now that we've got the raw data, let's try a simple experiment: **To what extent do replies reuse language from the speech they're replying to?**\n",
    "\n",
    "For this test:\n",
    "\n",
    "1. We'll consider as replies those speeches that come second in their conversation, i.e. `part==2`.\n",
    "2. We'll measure language reuse as lemmata shared between part 1 and part 2 of a speech cluster, as a fraction of the lemmata in part 1. We'll count only distinct *types*, that is, duplicate lemmata in a single speech won't be counted.\n",
    "3. To estimate how much overlap we might expect by chance, we'll also compare reply speeches to some randomly selected initial speeches from unrelated conversations.\n",
    "\n",
    "### Replies\n",
    "\n",
    "First, let's gather all speeches whose `part` attribute is `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by part\n",
    "replies = speeches.filterParts([2])\n",
    "\n",
    "# how many results?\n",
    "print(len(replies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incipits\n",
    "\n",
    "Now, let's gather all speeches whose `part` is `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by part\n",
    "incipits = speeches.filterParts([1])\n",
    "\n",
    "# how many results\n",
    "print(len(incipits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot more initial speeches than replies because so many speech clusters only have a single speech. Let's further limit the incipits under consideration to those from clusters represented in `replies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by clusters present in replies\n",
    "incipits = incipits.filterClusters(replies.getClusters())\n",
    "\n",
    "# how many results?\n",
    "print(len(incipits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to organize these two groups in two different ways. In one treatment, each reply is paired with its incipit. In the control, each reply is paired with a random incipit.\n",
    "\n",
    "### Incipit-reply matched pairs\n",
    "\n",
    "Let's try pairing them off, first, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with empty list\n",
    "reply_pairs = []\n",
    "\n",
    "# check each reply\n",
    "for reply in replies:\n",
    "\n",
    "    # get incipits with same cluster id\n",
    "    results = incipits.filterClusters([reply.cluster])\n",
    "    \n",
    "    # should be only one\n",
    "    if len(results) == 0:\n",
    "        print(f'found no incipit for cluster {reply.cluster.id}')\n",
    "    elif len(results) > 1:\n",
    "        print(f'found {len(results)} incipits for cluster {reply.cluster.id}')\n",
    "    else:\n",
    "        reply_pairs.append((results[0], reply))\n",
    "        \n",
    "\n",
    "# how many pairs?\n",
    "print(len(reply_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized control set\n",
    "\n",
    "Now let's shuffle the incipits and replies to create some control pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with empty list\n",
    "random_pairs = []\n",
    "\n",
    "# choose incipts, replies at random\n",
    "for rep in range(1000):\n",
    "    i = random.randint(0, len(incipits)-1)\n",
    "    j = random.randint(0, len(replies)-1)\n",
    "    if incipits[i].cluster.id != replies[j].cluster.id:\n",
    "        random_pairs.append((incipits[i], replies[j]))\n",
    "\n",
    "# how many pairs?\n",
    "print(len(random_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric for shared lemmata\n",
    "\n",
    "For a given pair of speeches, one incipit and one reply, we're looking for the number of unique, shared lemmata divided by the number of unique lemmata in the incipit alone.\n",
    "\n",
    "First, we'll write a custom function to return the shared lemmata (ommitting punctuation, which CLTK assigns to the lemma \"punc\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared(speech_a, speech_b, inc_punc=False):\n",
    "    '''Return shared lemmata between two speeches'''\n",
    "    \n",
    "    shared = set([lem for lem in speech_a.lemmata if lem in speech_b.lemmata])\n",
    "    \n",
    "    if not inc_punc:\n",
    "        if 'punc' in shared:\n",
    "            shared = set([lem for lem in shared if lem != 'punc'])\n",
    "    \n",
    "    return shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of incipit-reply pairs to randomized control\n",
    "\n",
    "Now we just run through out two groups of pairs, and calculate the metric for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with empty list\n",
    "nshared_reply = []\n",
    "\n",
    "# calculate for matched pairs\n",
    "for incipit, reply in reply_pairs:\n",
    "    nshared_reply.append(len(shared(incipit, reply))/len(set(incipit.lemmata)))\n",
    "\n",
    "# how many values?\n",
    "print(len(nshared_reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with empty list\n",
    "nshared_random = []\n",
    "\n",
    "# calculate for matched pairs\n",
    "for incipit, reply in random_pairs:\n",
    "    nshared_random.append(len(shared(incipit, reply))/len(set(incipit.lemmata)))\n",
    "\n",
    "# how many values?\n",
    "print(len(nshared_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "Are the two groups similar? Let's use a simply box and whisker plot to get an overview of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot([nshared_reply, nshared_random], notch=True, labels=['matched pairs', 'random pairs'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging a little deeper...\n",
    "\n",
    "Well, the results are suggestive, but how significant are they. One way forward might be a statistical analysis of the distributions of this metric in the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.hist(nshared_reply)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.hist(nshared_random)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(nshared_reply, nshared_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
