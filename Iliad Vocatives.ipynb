{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235598d1",
   "metadata": {},
   "source": [
    "# 1. General notes\n",
    "\n",
    "### Local CTS server\n",
    "\n",
    "The public Scaife CTS server from Perseus doesn't provide Quintus. The text exists in the [canonical-greekLit](https://github.com/PerseusDL/canonical-greekLit) Git repo, but it's not configured for Nautilus to serve it. I created my own [canonical-greekLit fork](https://github.com/cwf2/canonical-greekLit) and edited Quintus until Nautilus was happy.\n",
    "\n",
    "Run the following code in a terminal window to install and run the server locally.\n",
    "\n",
    "```bash\n",
    "    git clone https://github.com/cwf2/canonical-greekLit\n",
    "    capitains-nautilus canonical-greekLit --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c652b7",
   "metadata": {},
   "source": [
    "### Odyssey variant reading\n",
    "\n",
    "The DICES database has a speech by Circe to Odysseus beginning at Od. 10.456; but in the Perseus edition, 456 is missing and the speech begins at 457. I've manually changed the speech start line here to agree with Perseus, avoiding an error when we download the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789d6df",
   "metadata": {},
   "source": [
    "# 2. Steps for processing the speeches\n",
    "\n",
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "from dicesapi.text import CtsAPI\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78ba62",
   "metadata": {},
   "source": [
    "## Initialize connections to DICES and CTS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to the database\n",
    "api = DicesAPI(logfile='dices.log')\n",
    "\n",
    "# initialize connection to digital libraries\n",
    "cts = CtsAPI(\n",
    "    dices_api = api,\n",
    "    servers = {\n",
    "        # None:  'https://scaife-cts.perseus.org/api/cts', # default\n",
    "        None: 'http://localhost:5000/cts', # use local server\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# cache file for saving parsed text\n",
    "pickle_file = os.path.join('..', 'data', '{work}_speeches.pickle')\n",
    "# csv file for export to Excel\n",
    "csv_file = os.path.join('..', 'data', '{work}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4a362",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "I'm setting this up the steps as a series of function definitions so that it's easier to loop over the individual texts.\n",
    "\n",
    "### Download the speech metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlSpeechData(work):\n",
    "    '''Download all the speeches for a given work'''\n",
    "\n",
    "    print(f'Retrieving speeches for {work}')\n",
    "            \n",
    "    speeches = api.getSpeeches(work_title=work.title())\n",
    "    print('retrieved', len(speeches), 'results')\n",
    "\n",
    "    \n",
    "    # cludge for textual variant in Odyssey\n",
    "    if work.title() == 'Odyssey':\n",
    "        for s in speeches:\n",
    "            if s.l_fi == '10.456':\n",
    "                s.l_fi = '10.457'\n",
    "    \n",
    "    # another cludge to remove the apologia\n",
    "    if work.title() == 'Odyssey':\n",
    "        speeches = [s for s in speeches if s.l_fi.split('.')[0] == s.l_la.split('.')[0]]\n",
    "                \n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aff097",
   "metadata": {},
   "source": [
    "### Download the text of the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a850bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlSpeechText(speeches):\n",
    "    '''Download the text of the speeches from CTS server, append to speech objects'''\n",
    "    pbar = NotebookPBar(max=len(speeches), prefix='Downloading text')\n",
    "\n",
    "    for s in speeches:\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            s.passage = cts.getPassage(s)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5ca3b",
   "metadata": {},
   "source": [
    "### Parse the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSpeechText(speeches):\n",
    "    '''Run CLTK NLP pipeline to parse all the speeches'''\n",
    "    \n",
    "    pbar = NotebookPBar(max=len(speeches), prefix='Running NLP')\n",
    "\n",
    "    for s in speeches:\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            print('no passage:', s)\n",
    "        elif not hasattr(s.passage, 'cltk') or s.passage.cltk is None:\n",
    "            s.passage.runCltkPipeline(remove_punct=True)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd01270",
   "metadata": {},
   "source": [
    "### Format tokens as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTokenTable(speeches):\n",
    "    '''Create a DataFrame with one row per token'''\n",
    "    words = pd.DataFrame(dict(\n",
    "        speech_id = s.id,\n",
    "        book = s.l_fi.split('.')[0],\n",
    "        line = s.passage.line_array[s.passage.getLineIndex(w)]['n'],\n",
    "        l_ind = s.passage.getLineIndex(w)+1,\n",
    "        spkr = s.getSpkrString(),\n",
    "        addr = s.getAddrString(),\n",
    "        gend_spkr = ','.join(sorted(set(inst.gender for inst in s.spkr))),\n",
    "        gend_addr = ','.join(sorted(set(inst.gender for inst in s.addr))),\n",
    "        string = w.string,\n",
    "        lemma = w.lemma,\n",
    "        upos = w.upos,\n",
    "        is_voc = 'vocative' in str(w.features),\n",
    "        features = str(w.features),\n",
    "    ) for s in speeches for w in s.passage.cltk)\n",
    "\n",
    "    # filter out punctuation tokens\n",
    "    words = words[(words.string != '.') & (words.upos != 'PUNCT')]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec746b",
   "metadata": {},
   "source": [
    "### Run the whole workflow on a specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWorkflow(work):\n",
    "    '''Run all the previous functions in order on one text'''\n",
    "\n",
    "    print('Processing', work.title())\n",
    "    \n",
    "    # use cached data if present\n",
    "    cache = pickle_file.format(work=work)\n",
    "    if os.path.exists(cache):\n",
    "        with open(cache, 'rb') as f:\n",
    "            speeches = pickle.load(f)\n",
    "        print('loaded', len(speeches), 'cached results')\n",
    "    else:    \n",
    "        speeches = dlSpeechData(work)\n",
    "        dlSpeechText(speeches)\n",
    "        parseSpeechText(speeches)\n",
    "        with open(cache, 'wb') as f:\n",
    "            pickle.dump(speeches, f)\n",
    "        print('saved', len(speeches), 'results to', cache)\n",
    "    \n",
    "    # generate tabular data\n",
    "    words = makeTokenTable(speeches)\n",
    "    \n",
    "    # save output\n",
    "    output = csv_file.format(work=work)\n",
    "    print(f'Writing {output}')\n",
    "    words.to_csv(output, index=False)\n",
    "    \n",
    "    # return the table\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84b482",
   "metadata": {},
   "source": [
    "# 3. Try it out\n",
    "\n",
    "## process one text\n",
    "\n",
    "### select work\n",
    "\n",
    "Change this to one of `'iliad'`, `'odyssey'`, or `'posthomerica'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = 'iliad'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6900376",
   "metadata": {},
   "source": [
    "### run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = runWorkflow(work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae26c69",
   "metadata": {},
   "source": [
    "### inspect the table of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431139",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3b2d3",
   "metadata": {},
   "source": [
    "## summary statistics\n",
    "\n",
    "### vocatives by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[words.is_voc].pivot_table(\n",
    "    index = 'book',\n",
    "    values = 'speech_id',\n",
    "    aggfunc = 'count',\n",
    "    sort = False,\n",
    "    fill_value = 0,\n",
    ").plot.bar(\n",
    "    title = f'Vocatives in the {work.title()}',\n",
    "    legend = False,\n",
    "    rot = False,\n",
    "    ylabel = 'count',\n",
    "    figsize = (10,4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235cc99",
   "metadata": {},
   "source": [
    "### Normalized for book length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b95867",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_book = words.pivot_table(\n",
    "    index = 'book',\n",
    "    values = 'speech_id',\n",
    "    columns = 'is_voc',\n",
    "    aggfunc = 'count',\n",
    "    sort = False,\n",
    "    fill_value = 0,\n",
    ").rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "voc_book['prop'] = voc_book['voc'] / (voc_book['voc'] + voc_book['other']) * 1000\n",
    "\n",
    "voc_book['prop'].plot.bar(\n",
    "    title = f'Vocatives in the {work.title()}',\n",
    "    legend = False,\n",
    "    ylabel = 'count per 1000 words',\n",
    "    rot = False,\n",
    "    figsize = (10,4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1b6ce",
   "metadata": {},
   "source": [
    "### by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_spkr = words.pivot_table(\n",
    "    index = 'spkr',\n",
    "    values = 'speech_id',\n",
    "    columns = 'is_voc',\n",
    "    aggfunc = 'count',\n",
    "    fill_value = 0,\n",
    ")\n",
    "\n",
    "voc_spkr = voc_spkr.rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "voc_spkr['prop'] = round(voc_spkr['voc'] / (voc_spkr['voc'] + voc_spkr['other']) * 1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f429a",
   "metadata": {},
   "source": [
    "#### greatest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(voc_spkr.sort_values('voc', ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683ba9f",
   "metadata": {},
   "source": [
    "#### highest proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d884ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(voc_spkr.sort_values('prop', ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf33575",
   "metadata": {},
   "source": [
    "#### highest proportion among speakers of at least 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(voc_spkr[(voc_spkr.other + voc_spkr.voc) > 999].sort_values('prop', ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218fedb3",
   "metadata": {},
   "source": [
    "### by part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc36ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_pos = words.pivot_table(\n",
    "    index = 'upos',\n",
    "    values = 'speech_id',\n",
    "    columns = 'is_voc',\n",
    "    aggfunc = 'count',\n",
    "    fill_value = 0,\n",
    ").rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "voc_pos.sort_values('voc', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
