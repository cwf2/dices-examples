{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235598d1",
   "metadata": {},
   "source": [
    "# 1. General notes\n",
    "\n",
    "### Local CTS server\n",
    "\n",
    "The public Scaife CTS server from Perseus doesn't provide Quintus. The text exists in the [canonical-greekLit](https://github.com/PerseusDL/canonical-greekLit) Git repo, but it's not configured for Nautilus to serve it. I created my own [canonical-greekLit fork](https://github.com/cwf2/canonical-greekLit) and edited Quintus until Nautilus was happy.\n",
    "\n",
    "This notebook should come with cached data, so you don't need to reprocess the texts. If you do want to replicate everything from scratch, then run the following code in a terminal window to install and run the CTS server locally.\n",
    "\n",
    "```bash\n",
    "    git clone https://github.com/cwf2/canonical-greekLit\n",
    "    capitains-nautilus canonical-greekLit --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c652b7",
   "metadata": {},
   "source": [
    "### Odyssey variant reading\n",
    "\n",
    "The DICES database has a speech by Circe to Odysseus beginning at Od. 10.456; but in the Perseus edition, 456 is missing and the speech begins at 457. I've manually changed the speech start line here to agree with Perseus, avoiding an error when we download the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789d6df",
   "metadata": {},
   "source": [
    "# 2. Steps for processing the speeches\n",
    "\n",
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "from dicesapi.text import CtsAPI\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, widgets\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78ba62",
   "metadata": {},
   "source": [
    "## Initialize connections to DICES and CTS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to the database\n",
    "api = DicesAPI(logfile='dices.log')\n",
    "\n",
    "# initialize connection to digital libraries\n",
    "cts = CtsAPI(\n",
    "    dices_api = api,\n",
    "    servers = {\n",
    "        # None:  'https://scaife-cts.perseus.org/api/cts', # default\n",
    "        None: 'http://localhost:5000/cts', # use local server\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4a362",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "I'm setting this up the steps as a series of function definitions so that it's easier to loop over the individual texts.\n",
    "\n",
    "### Download the speech metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlSpeechData(work):\n",
    "    '''Download all the speeches for a given work'''\n",
    "\n",
    "    print(f'Retrieving speeches for {work}')\n",
    "            \n",
    "    speeches = api.getSpeeches(work_title=work.title())\n",
    "    print('retrieved', len(speeches), 'results')\n",
    "\n",
    "    \n",
    "    # cludge for textual variant in Odyssey\n",
    "    if work.title() == 'Odyssey':\n",
    "        for s in speeches:\n",
    "            if s.l_fi == '10.456':\n",
    "                s.l_fi = '10.457'\n",
    "    \n",
    "    # another cludge to remove the apologia\n",
    "    if work.title() == 'Odyssey':\n",
    "        speeches = [s for s in speeches if s.l_fi.split('.')[0] == s.l_la.split('.')[0]]\n",
    "                \n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aff097",
   "metadata": {},
   "source": [
    "### Download the text of the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a850bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlSpeechText(speeches):\n",
    "    '''Download the text of the speeches from CTS server, append to speech objects'''\n",
    "    pbar = NotebookPBar(max=len(speeches), prefix='Downloading text')\n",
    "\n",
    "    for s in speeches:\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            s.passage = cts.getPassage(s)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5ca3b",
   "metadata": {},
   "source": [
    "### Parse the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSpeechText(speeches):\n",
    "    '''Run CLTK NLP pipeline to parse all the speeches'''\n",
    "    \n",
    "    pbar = NotebookPBar(max=len(speeches), prefix='Running NLP')\n",
    "\n",
    "    for s in speeches:\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            print('no passage:', s)\n",
    "        elif not hasattr(s.passage, 'cltk') or s.passage.cltk is None:\n",
    "            s.passage.runCltkPipeline(remove_punct=True)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd01270",
   "metadata": {},
   "source": [
    "### Format tokens as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTokenTable(speeches):\n",
    "    '''Create a DataFrame with one row per token'''\n",
    "    words = pd.DataFrame(dict(\n",
    "        speech_id = s.id,\n",
    "        book = s.l_fi.split('.')[0],\n",
    "        line = s.passage.line_array[s.passage.getLineIndex(w)]['n'],\n",
    "        l_ind = s.passage.getLineIndex(w)+1,\n",
    "        spkr = s.getSpkrString(),\n",
    "        addr = s.getAddrString(),\n",
    "        gend_spkr = ','.join(sorted(set(inst.gender for inst in s.spkr))),\n",
    "        gend_addr = ','.join(sorted(set(inst.gender for inst in s.addr))),\n",
    "        string = w.string,\n",
    "        lemma = w.lemma,\n",
    "        upos = w.upos,\n",
    "        is_voc = 'vocative' in str(w.features),\n",
    "        features = str(w.features),\n",
    "    ) for s in speeches for w in s.passage.cltk)\n",
    "\n",
    "    # filter out punctuation tokens\n",
    "    words = words[(words.string != '.') & (words.upos != 'PUNCT')]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec746b",
   "metadata": {},
   "source": [
    "## 3. Run the whole workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3292a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {}\n",
    "words = {}\n",
    "works = ['iliad', 'odyssey', 'posthomerica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for work in works:\n",
    "    print('Processing', work.title())\n",
    "    \n",
    "    # local file paths\n",
    "    cache = os.path.join('data', f'{work}_speeches.pickle')\n",
    "    output = os.path.join('data', f'{work}.csv')\n",
    "    \n",
    "    # use cached data if present\n",
    "    if os.path.exists(cache):\n",
    "        with open(cache, 'rb') as f:\n",
    "            speeches[work] = pickle.load(f)\n",
    "        print('loaded', len(speeches[work]), 'cached results')\n",
    "    else:    \n",
    "        speeches[work] = dlSpeechData(work)\n",
    "        dlSpeechText(speeches[work])\n",
    "        parseSpeechText(speeches[work])\n",
    "        with open(cache, 'wb') as f:\n",
    "            pickle.dump(speeches[work], f)\n",
    "        print('saved', len(speeches[work]), 'results to', cache)\n",
    "    \n",
    "    # generate tabular data\n",
    "    words[work] = makeTokenTable(speeches[work])\n",
    "    \n",
    "    # save output\n",
    "    print(f'Writing {output}')\n",
    "    words[work].to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae26c69",
   "metadata": {},
   "source": [
    "### inspect the table of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431139",
   "metadata": {},
   "outputs": [],
   "source": [
    "words['iliad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3b2d3",
   "metadata": {},
   "source": [
    "# 4. summary statistics\n",
    "\n",
    "## helper functions\n",
    "\n",
    "### Simple count of vocatives by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByBook(work):\n",
    "    df = words[work][words[work].is_voc].pivot_table(\n",
    "        index = 'book',\n",
    "        values = 'speech_id',\n",
    "        aggfunc = 'count',\n",
    "        sort = False,\n",
    "        fill_value = 0,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def getPlotByBook(work):\n",
    "    df = tableByBook(work)\n",
    "    plot = df.plot.bar(\n",
    "        title = f'Vocatives in the {work.title()}',\n",
    "        legend = False,\n",
    "        rot = False,\n",
    "        ylabel = 'count',\n",
    "        figsize = (10,4),\n",
    "    )\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235cc99",
   "metadata": {},
   "source": [
    "### Normalized for book length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b95867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByBookNorm(work):\n",
    "\n",
    "    df = words[work].pivot_table(\n",
    "        index = 'book',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        sort = False,\n",
    "        fill_value = 0,\n",
    "    ).rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df['prop'] = df['voc'] / (df['voc'] + df['other']) * 1000\n",
    "\n",
    "    return df\n",
    "\n",
    "def getPlotByBookNorm(work):\n",
    "    df = getTableByBookNorm(work)\n",
    "    plot = df['prop'].plot.bar(\n",
    "        title = f'Vocatives in the {work.title()}',\n",
    "        legend = False,\n",
    "        ylabel = 'count per 1000 words',\n",
    "        rot = False,\n",
    "        figsize = (10,4),\n",
    "    )\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1b6ce",
   "metadata": {},
   "source": [
    "### by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableBySpeaker(work):\n",
    "    \n",
    "    df = words[work].pivot_table(\n",
    "        index = 'spkr',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df['prop'] = round(df['voc'] / (df['voc'] + df['other']) * 1000, 2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189a14",
   "metadata": {},
   "source": [
    "### by part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415252f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByPOS(work):\n",
    "\n",
    "    df = words[work].pivot_table(\n",
    "        index = 'upos',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    ).rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df.sort_values('voc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e10a8",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "### Normalized vocatives by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interactive(getPlotByBookNorm, work=works))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815de963",
   "metadata": {},
   "source": [
    "### By Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work: display(getTableBySpeaker(work))\n",
    "interactive(view, work=works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f429a",
   "metadata": {},
   "source": [
    "#### greatest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work, n: display(getTableBySpeaker(work).sort_values('voc', ascending=False)[:n])\n",
    "display(interactive(view, work=works, n=widgets.IntSlider(min=10, max=100, step=5, value=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683ba9f",
   "metadata": {},
   "source": [
    "#### highest proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d884ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work, n: display(getTableBySpeaker(work).sort_values('prop', ascending=False)[:n])\n",
    "display(interactive(view, work=works, n=widgets.IntSlider(min=10, max=100, step=5, value=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf33575",
   "metadata": {},
   "source": [
    "#### highest proportion among speakers of at least 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(work, min_words, n_results):\n",
    "    df = getTableBySpeaker(work)\n",
    "    df = df[(df.other + df.voc) >= min_words]\n",
    "    df = df.sort_values('prop', ascending=False)[:n_results]\n",
    "    display(df)\n",
    "\n",
    "display(interactive(view, work=works, n_results=widgets.IntSlider(min=10, max=100, step=5, value=10),\n",
    "                   min_words = widgets.IntSlider(min=100, max=2000, step=100, value=1000)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
